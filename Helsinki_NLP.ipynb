{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "mount_file_id": "1yjTB0V7MAfQN_xVaLIcinT9uDqtVt7fU",
      "authorship_tag": "ABX9TyNPdtDx2/pYb2w/PXdSJFyJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yiyc-kr/llm-study/blob/main/Helsinki_NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sacremoses"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "6pqV18wUelDg",
        "outputId": "dfa116a4-4082-46b8-f362-0b5a75d65a54"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.1.1-py3-none-any.whl (897 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.5/897.5 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacremoses) (2024.5.15)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from sacremoses) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from sacremoses) (1.4.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sacremoses) (4.66.4)\n",
            "Installing collected packages: sacremoses\n",
            "Successfully installed sacremoses-0.1.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "YAtcKKo7dOK_"
      },
      "outputs": [],
      "source": [
        "from transformers import MarianMTModel, MarianTokenizer, GenerationConfig\n",
        "\n",
        "# MarianMT 모델 이름 (예시: 한국어에서 영어로 번역)\n",
        "model_name = \"Helsinki-NLP/opus-mt-ko-en\"\n",
        "\n",
        "# 토크나이저와 모델 불러오기\n",
        "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
        "model = MarianMTModel.from_pretrained(model_name)\n",
        "\n",
        "# 생성 매개변수 설정\n",
        "generation_config = GenerationConfig.from_pretrained(model_name)\n",
        "generation_config.max_length = 512\n",
        "generation_config.num_beams = 6\n",
        "generation_config.bad_words_ids = [[65000]]\n",
        "generation_config.forced_eos_token_id = 0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 번역할 텍스트\n",
        "text = \"고객기념일\"\n",
        "\n",
        "# 입력 텍스트를 토큰화\n",
        "inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "\n",
        "# 모델을 사용하여 번역 생성\n",
        "translated = model.generate(**inputs)\n",
        "\n",
        "# 번역된 텍스트 디코딩\n",
        "translated_text = tokenizer.decode(translated[0], skip_special_tokens=True)\n",
        "\n",
        "print(translated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kESgDTVydgou",
        "outputId": "ded58e39-fd00-4228-8ed4-d2f5fe7c3948"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Custom Day\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "local_model_directory = \"drive/MyDrive/llm-study/Helsinki-NLP-opus-mt-ko-en\""
      ],
      "metadata": {
        "id": "YD6-D9SkeNfL"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(local_model_directory)\n",
        "tokenizer.save_pretrained(local_model_directory)\n",
        "generation_config.save_pretrained(local_model_directory)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JueL_5MdeRh7",
        "outputId": "408c7905-b88f-46dc-cb34-cc546c5fe32c"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
            "Non-default generation parameters: {'max_length': 512, 'num_beams': 6, 'bad_words_ids': [[65000]], 'forced_eos_token_id': 0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import MarianMTModel, MarianTokenizer, GenerationConfig\n",
        "\n",
        "# 로컬 모델 디렉토리 경로\n",
        "# local_model_directory = \"local_model_directory\"\n",
        "\n",
        "# 로컬 디렉토리에서 토크나이저, 모델, GenerationConfig 불러오기\n",
        "tokenizer = MarianTokenizer.from_pretrained(local_model_directory)\n",
        "model = MarianMTModel.from_pretrained(local_model_directory)\n",
        "generation_config = GenerationConfig.from_pretrained(local_model_directory)\n",
        "\n",
        "# 번역할 텍스트\n",
        "text = \"안녕하세요, 어떻게 지내세요?\"\n",
        "\n",
        "# 입력 텍스트를 토큰화\n",
        "inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "\n",
        "# 모델을 사용하여 번역 생성\n",
        "translated = model.generate(**inputs, generation_config=generation_config)\n",
        "\n",
        "# 번역된 텍스트 디코딩\n",
        "translated_text = tokenizer.decode(translated[0], skip_special_tokens=True)\n",
        "\n",
        "print(translated_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FN4TXoZggJxH",
        "outputId": "6097e667-523d-4970-f791-125dca495d38"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello. How are you?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "local_model_directory"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "ZnzDNx-VgO1I",
        "outputId": "c0919ce4-80ad-431e-adc1-ccab525cdfc6"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'drive/MyDrive/llm-study/Helsinki-NLP-opus-mt-ko-en'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!tar -zcvf drive/MyDrive/llm-study/Helsinki-NLP-opus-mt-ko-en.tar.gz drive/MyDrive/llm-study/Helsinki-NLP-opus-mt-ko-en"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xNSPo3I4gUU-",
        "outputId": "ac8db00b-94b8-476e-b368-5db8e8416180"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "drive/MyDrive/llm-study/Helsinki-NLP-opus-mt-ko-en/\n",
            "drive/MyDrive/llm-study/Helsinki-NLP-opus-mt-ko-en/config.json\n",
            "tar: drive/MyDrive/llm-study/Helsinki-NLP-opus-mt-ko-en/config.json: file changed as we read it\n",
            "drive/MyDrive/llm-study/Helsinki-NLP-opus-mt-ko-en/generation_config.json\n",
            "tar: drive/MyDrive/llm-study/Helsinki-NLP-opus-mt-ko-en/generation_config.json: file changed as we read it\n",
            "drive/MyDrive/llm-study/Helsinki-NLP-opus-mt-ko-en/model.safetensors\n",
            "drive/MyDrive/llm-study/Helsinki-NLP-opus-mt-ko-en/tokenizer_config.json\n",
            "tar: drive/MyDrive/llm-study/Helsinki-NLP-opus-mt-ko-en/tokenizer_config.json: file changed as we read it\n",
            "drive/MyDrive/llm-study/Helsinki-NLP-opus-mt-ko-en/special_tokens_map.json\n",
            "tar: drive/MyDrive/llm-study/Helsinki-NLP-opus-mt-ko-en/special_tokens_map.json: file changed as we read it\n",
            "drive/MyDrive/llm-study/Helsinki-NLP-opus-mt-ko-en/vocab.json\n",
            "tar: drive/MyDrive/llm-study/Helsinki-NLP-opus-mt-ko-en/vocab.json: file changed as we read it\n",
            "drive/MyDrive/llm-study/Helsinki-NLP-opus-mt-ko-en/source.spm\n",
            "tar: drive/MyDrive/llm-study/Helsinki-NLP-opus-mt-ko-en/source.spm: file changed as we read it\n",
            "drive/MyDrive/llm-study/Helsinki-NLP-opus-mt-ko-en/target.spm\n",
            "tar: drive/MyDrive/llm-study/Helsinki-NLP-opus-mt-ko-en/target.spm: file changed as we read it\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EyJsc4JBgaV2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}